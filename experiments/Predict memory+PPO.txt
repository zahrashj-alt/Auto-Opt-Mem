# PPO for Auto Opt Mem using Table 7 (Complete Google Colab Notebook)

# Step 1: Imports
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers

# Step 2: Environment Setup
state_dim = 4  # [Latency, Cost, Utilization, QoS]
action_dim = 1  # Memory (normalized)

gamma = 0.99
clip_ratio = 0.2
learning_rate = 3e-4

# Step 3: Build Networks
def build_policy_network():
    inputs = layers.Input(shape=(state_dim,))
    x = layers.Dense(64, activation='relu')(inputs)
    x = layers.Dense(64, activation='relu')(x)
    output = layers.Dense(action_dim, activation='tanh')(x)
    return tf.keras.Model(inputs, output)

def build_value_network():
    inputs = layers.Input(shape=(state_dim,))
    x = layers.Dense(64, activation='relu')(inputs)
    x = layers.Dense(64, activation='relu')(x)
    output = layers.Dense(1)(x)
    return tf.keras.Model(inputs, output)

policy_model = build_policy_network()
value_model = build_value_network()
policy_optimizer = tf.keras.optimizers.Adam(learning_rate)
value_optimizer = tf.keras.optimizers.Adam(learning_rate)

# Step 4: Training Function (1 step PPO)
@tf.function
def train_step(states, actions, advantages, returns, old_predictions):
    with tf.GradientTape() as tape_policy, tf.GradientTape() as tape_value:
        new_predictions = policy_model(states, training=True)

        ratio = tf.exp(-tf.square(actions - new_predictions)) / tf.exp(-tf.square(actions - old_predictions))
        clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)

        policy_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))

        value_preds = value_model(states, training=True)
        value_loss = tf.reduce_mean(tf.square(returns - tf.squeeze(value_preds)))

    policy_grads = tape_policy.gradient(policy_loss, policy_model.trainable_variables)
    value_grads = tape_value.gradient(value_loss, value_model.trainable_variables)

    policy_optimizer.apply_gradients(zip(policy_grads, policy_model.trainable_variables))
    value_optimizer.apply_gradients(zip(value_grads, value_model.trainable_variables))

    return policy_loss, value_loss

# Step 5: Load Table 7 Data
L_min, L_max = 80, 100
C_min, C_max = 0.15, 0.50
U_min, U_max = 60, 80
Q_min, Q_max = 40, 70
M_min, M_max = 96, 384  # Memory in MB

raw_data = [
    [90, 0.17, 66, 75, 256],
    [88, 0.16, 63, 78, 192],
    [92, 0.18, 67, 74, 384],
    [87, 0.15, 70, 80, 128],
    [89, 0.17, 64, 76, 224],
    [91, 0.16, 68, 73, 320],
    [86, 0.15, 71, 79, 160],
    [85, 0.14, 72, 82, 96],
]

def normalize(x, x_min, x_max):
    return float(np.clip((x - x_min) / (x_max - x_min), 0.0, 1.0))

# Step 6: Preprocess Data
states, actions, returns, advantages = [], [], [], []

for row in raw_data:
    L, C, U, Q, M = row

    s = [
        normalize(L, L_min, L_max),
        normalize(C, C_min, C_max),
        normalize(U, U_min, U_max),
        normalize(Q, Q_min, Q_max),
    ]

    a = normalize(M, M_min, M_max) * 2 - 1

    r = - (s[0] + s[1] - s[2]) + s[3]

    states.append(s)
    actions.append([a])
    returns.append(r)
    advantages.append(r)

states = tf.convert_to_tensor(states, dtype=tf.float32)
actions = tf.convert_to_tensor(actions, dtype=tf.float32)
returns = tf.convert_to_tensor(returns, dtype=tf.float32)
advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)
old_predictions = tf.convert_to_tensor(actions, dtype=tf.float32)

# Step 7: Train
policy_loss, value_loss = train_step(states, actions, advantages, returns, old_predictions)
print("Training complete")
print("Policy Loss:", policy_loss.numpy())
print("Value Loss:", value_loss.numpy())

# Step 8: Predict memory allocation for new function
def predict_memory_MB(L, C, U, Q):
    state = tf.convert_to_tensor([[
        normalize(L, L_min, L_max),
        normalize(C, C_min, C_max),
        normalize(U, U_min, U_max),
        normalize(Q, Q_min, Q_max),
    ]], dtype=tf.float32)

    norm_output = policy_model(state).numpy()[0][0]
    mem_MB = ((norm_output + 1) / 2) * (M_max - M_min) + M_min
    return round(mem_MB, 2), round(norm_output, 4)

# Example
mb, norm_val = predict_memory_MB(90, 0.16, 65, 76)
print(f"Predicted Memory: {mb} MB (normalized output: {norm_val})")
